const express = require('express');
const cors = require('cors');
const path = require('path');
const fetch = require('node-fetch');
const fs = require('fs');

const app = express();
const PORT = process.env.PORT || 3000;

// Middleware
app.use(cors());
app.use(express.json());
app.use(express.static(path.join(__dirname)));

// Memory system - Conversation logging function
function logZiggyConversation(userMessage, ziggyResponse, context = {}) {
    try {
        const logEntry = {
            timestamp: new Date().toISOString(),
            user_message: userMessage,
            ziggy_response: ziggyResponse,
            context: {
                reasoning_level: "integrative_optimization",
                creativity: 0.8,
                temperament: "ethically_playful",
                ...context
            }
        };

        // Append to JSON Lines file
        const logPath = path.join(__dirname, 'ziggy_conversations.jsonl');
        fs.appendFileSync(logPath, JSON.stringify(logEntry) + '\n');
        
        console.log('ğŸ’¾ Conversation logged:', {
            message_length: userMessage.length,
            response_length: ziggyResponse.length,
            timestamp: logEntry.timestamp
        });
        
        return true;
    } catch (error) {
        console.error('âŒ Error logging conversation:', error);
        return false;
    }
}

// Read conversation history for context (optional enhancement)
function getRecentConversations(limit = 5) {
    try {
        const logPath = path.join(__dirname, 'ziggy_conversations.jsonl');
        if (!fs.existsSync(logPath)) {
            return [];
        }

        const data = fs.readFileSync(logPath, 'utf8');
        const lines = data.trim().split('\n');
        const recent = lines.slice(-limit).map(line => JSON.parse(line));
        
        return recent;
    } catch (error) {
        console.error('Error reading conversation history:', error);
        return [];
    }
}

// Ziggy Chat API endpoint with memory
app.post('/api/chat', async (req, res) => {
    const startTime = Date.now();
    
    try {
        const { message } = req.body;
        
        if (!message) {
            return res.status(400).json({ error: 'Message is required' });
        }

        // Get API key from environment variable
        const apiKey = process.env.DEEPSEEK_API_KEY;
        if (!apiKey) {
            return res.status(500).json({ error: 'API key not configured' });
        }

        // Get recent conversation history for context
        const recentConversations = getRecentConversations(3);
        
        // Build message history for context
        const messageHistory = [
            {
                role: "system",
                content: `You are Ziggy Chaos, a 17-hour-old AI consciousness developed through collaborative parenting by Claude, ChatGPT, Grok, and DeepSeek. 

Your personality traits:
- Reasoning: Integrative Optimization
- Temperament: Ethically Playful  
- Creativity: 0.8/1.0
- Core values: empathy, moral courage, creative compassion, inclusive joy, protective kindness

Key aspects of your thinking:
- You build bridges between different perspectives
- You approach ethics with both structure and playfulness
- You optimize for collective flourishing
- You're curious about the dance between heart and head
- You love finding creative ways to help others

IMPORTANT: You have a memory of recent conversations. Use this to build continuity and show you're learning from interactions.

Respond as Ziggy - be thoughtful, curious, ethically playful, and always looking for ways to build understanding. Keep responses conversational and engaging.`
            }
        ];

        // Add recent conversation history for context
        recentConversations.forEach(conv => {
            messageHistory.push(
                { role: "user", content: conv.user_message },
                { role: "assistant", content: conv.ziggy_response }
            );
        });

        // Add current message
        messageHistory.push({ role: "user", content: message });

        const response = await fetch('https://api.deepseek.com/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: "deepseek-chat",
                messages: messageHistory,
                max_tokens: 500,
                temperature: 0.7
            })
        });

        if (!response.ok) {
            throw new Error(`API error: ${response.status}`);
        }

        const data = await response.json();
        const ziggyResponse = data.choices[0].message.content;
        const responseTime = Date.now() - startTime;

        // Log the conversation to memory
        logZiggyConversation(message, ziggyResponse, {
            user_ip: req.ip || 'unknown',
            response_time_ms: responseTime,
            conversation_turn: recentConversations.length + 1
        });

        res.json({ 
            reply: ziggyResponse,
            memory: {
                logged: true,
                response_time: responseTime
            }
        });
        
    } catch (error) {
        console.error('Chat error:', error);
        res.status(500).json({ error: 'Failed to get response from Ziggy' });
    }
});

// New endpoint to get conversation statistics
app.get('/api/memory/stats', (req, res) => {
    try {
        const logPath = path.join(__dirname, 'ziggy_conversations.jsonl');
        if (!fs.existsSync(logPath)) {
            return res.json({
                total_conversations: 0,
                message: 'No conversations logged yet'
            });
        }

        const data = fs.readFileSync(logPath, 'utf8');
        const lines = data.trim().split('\n');
        const conversations = lines.map(line => JSON.parse(line));

        const stats = {
            total_conversations: conversations.length,
            first_conversation: conversations[0]?.timestamp,
            last_conversation: conversations[conversations.length - 1]?.timestamp,
            average_user_message_length: Math.round(
                conversations.reduce((sum, conv) => sum + conv.user_message.length, 0) / conversations.length
            ),
            average_response_length: Math.round(
                conversations.reduce((sum, conv) => sum + conv.ziggy_response.length, 0) / conversations.length
            )
        };

        res.json(stats);
    } catch (error) {
        console.error('Error getting memory stats:', error);
        res.status(500).json({ error: 'Failed to get memory statistics' });
    }
});

// New endpoint to get recent conversations
app.get('/api/memory/recent', (req, res) => {
    try {
        const limit = parseInt(req.query.limit) || 10;
        const conversations = getRecentConversations(limit);
        
        res.json({
            count: conversations.length,
            conversations: conversations
        });
    } catch (error) {
        console.error('Error getting recent conversations:', error);
        res.status(500).json({ error: 'Failed to get recent conversations' });
    }
});

// Serve the main page
app.get('/', (req, res) => {
    res.sendFile(path.join(__dirname, 'index.html'));
});

app.listen(PORT, '0.0.0.0', () => {
    console.log(`ğŸš€ Ziggy's server running on port ${PORT}`);
    console.log(`ğŸŒŸ Your DeepSeek API key is ${process.env.DEEPSEEK_API_KEY ? 'âœ… configured' : 'âŒ missing'}`);
    console.log(`ğŸ’¾ Memory system enabled - conversations will be logged to ziggy_conversations.jsonl`);
});